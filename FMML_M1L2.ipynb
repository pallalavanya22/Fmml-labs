{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallalavanya22/Fmml-labs/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "6f379d6f-dc2c-427a-93f7-587d7b909a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "f71dd070-d8a0-41e1-b595-107fec9f47e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "73ffc0bd-354d-4051-f17a-c6adebdd58fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "2d0ed616-7ece-4d8f-8f8d-cb1eb48f7385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "ab09a4be-1b0e-4db2-ef67-a4b5f41c48fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "8e5506e9-474c-4fd6-d7a6-b14aa1b8b8d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "2487fee9-0efc-4ce9-8b76-c8fd90b0c94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "ANS.....:-\n",
        "     The accuracy of the validation set can be impacted by changes in the percentage of the data allocated for validation. Here’s what typically happens:\n",
        "\n",
        " 1. **Increasing the Percentage of the Validation Set:**\n",
        "   - **More Representative Validation:** With more data in the validation set, the model’s performance metrics (like accuracy) become more reliable because they are evaluated on a broader sample that better represents the overall dataset.\n",
        "   - **Potential Decrease in Training Performance:** However, increasing the validation set size reduces the amount of data available for training. This reduction in training data might hinder the model's learning, leading to poorer generalization and potentially lower validation accuracy.\n",
        "   - **Stabilization of Validation Metrics:** With a larger validation set, the accuracy and other metrics are less likely to fluctuate, providing more consistent and reliable performance measures.\n",
        "\n",
        " 2. **Reducing the Percentage of the Validation Set:**\n",
        "   - **Less Representative Validation:** With fewer data points in the validation set, the evaluation may become less representative, leading to less reliable accuracy metrics. The smaller sample size can introduce more noise and variability, making the model’s performance appear better or worse than it actually is.\n",
        "   - **Better Training Performance:** With more data available for training, the model may learn better and achieve higher performance on both the training and validation sets. However, this improved learning might not always translate to better generalization.\n",
        "   - **Increased Variability in Validation Metrics:** A smaller validation set can lead to more fluctuation in accuracy, especially if it contains outliers or is not well-balanced.\n",
        "\n",
        " Conclusion:\n",
        "Increasing the validation set size generally improves the reliability of the accuracy metric but may hurt model training due to less data being available for learning. Reducing the validation set size might enhance training but can lead to less reliable and more volatile validation metrics. A balanced approach is usually needed, often guided by cross-validation techniques to make the most of the available data.\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "ANS...:\n",
        "The accuracy of the validation set can be impacted by changes in the percentage of data allocated for validation. Here’s how:\n",
        "\n",
        "1. **Increasing the Percentage of the Validation Set:**\n",
        "   - **More Reliable Accuracy Measurement:** A larger validation set provides a better representation of the overall data distribution. This makes the validation accuracy more stable and less prone to fluctuations, leading to a more reliable estimate of model performance.\n",
        "   - **Less Training Data:** However, increasing the validation set reduces the amount of data available for training. With less training data, the model may not learn as effectively, potentially leading to poorer generalization and a decrease in validation accuracy.\n",
        "\n",
        " 2. **Reducing the Percentage of the Validation Set:**\n",
        "   - **Less Reliable Accuracy Measurement:** A smaller validation set means the accuracy may be more sensitive to the specific samples in that set. It could lead to less reliable and more variable accuracy measurements since a small subset may not fully represent the overall data distribution.\n",
        "   - **More Training Data:** More data is available for training, which can help the model learn better. This can potentially improve both training and validation accuracy, though this improved performance might be due to overfitting if the validation set is too small.\n",
        "\n",
        " Summary:\n",
        "- **Increasing the validation set size** generally provides a more reliable estimate of model accuracy but at the cost of reducing training data.\n",
        "- **Decreasing the validation set size** can improve training by providing more data, but it risks making the validation accuracy less reliable and prone to variability.\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "ANS...:\n",
        "The size of the training and validation sets directly impacts how well the validation set can predict the model's performance on the test set. Here’s how:\n",
        "\n",
        "1. **Training Set Size:**\n",
        "   - **Larger Training Set:**\n",
        "     - **Better Model Generalization:** A larger training set allows the model to learn more patterns from the data, improving its generalization ability. This leads to better performance on both the validation and test sets.\n",
        "     - **More Accurate Validation Performance:** With more data to train on, the model can perform better on unseen data, so the validation accuracy is a better reflection of how well the model will perform on the test set.\n",
        "   - **Smaller Training Set:**\n",
        "     - **Limited Learning:** A smaller training set may lead to underfitting because the model might not learn enough from the limited data. As a result, validation accuracy may be poor, and it may not be a reliable predictor of test set performance.\n",
        "\n",
        " 2. **Validation Set Size:**\n",
        "   - **Larger Validation Set:**\n",
        "     - **More Reliable Estimates of Test Accuracy:** A larger validation set provides a more representative sample of the data distribution, leading to more stable and accurate estimates of model performance. This improves the reliability of predicting how well the model will do on the test set.\n",
        "     - **Less Training Data:** However, a larger validation set reduces the training data, which can hurt model performance if too little data is left for training. This could cause the validation accuracy to underestimate the true test accuracy.\n",
        "   - **Smaller Validation Set:**\n",
        "     - **Less Reliable Estimates of Test Accuracy:** A smaller validation set may not be representative of the overall data distribution. This can lead to high variance in validation accuracy, making it a less reliable indicator of test set performance.\n",
        "     - **More Training Data:** With more data available for training, the model might learn better and achieve better performance, but this performance could be overestimated if the validation set is too small to capture the true variability of the data.\n",
        "\n",
        "Summary:\n",
        "- **Training Set Size:** A larger training set generally leads to better model generalization, allowing the validation set to more accurately predict test set performance.\n",
        "- **Validation Set Size:** A larger validation set provides more reliable estimates of test accuracy but at the cost of reducing training data. A smaller validation set increases the risk of inaccurate or volatile predictions for test performance.\n",
        "\n",
        "Balancing the size of the training and validation sets is crucial. Techniques like cross-validation can help mitigate the trade-offs by ensuring that all data is effectively used for both training and validation.\n",
        "Here’s how you can implement a 3-Nearest Neighbors (KNN) classifier and compare it with the 1-Nearest Neighbors classifier using scikit-learn:\n",
        "\n",
        " Step 1: Import Necessary Libraries\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "```\n",
        "\n",
        " Step 2: Implement and Compare the 1-Nearest and 3-Nearest Neighbors Classifiers\n",
        "\n",
        "```python\n",
        "# 1-Nearest Neighbor Classifier\n",
        "knn_1 = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_1.fit(alltraindata, alltrainlabel)\n",
        "testpred_knn1 = knn_1.predict(testdata)\n",
        "testAccuracy_knn1 = accuracy_score(testlabel, testpred_knn1)\n",
        "print(f\"Test accuracy using 1-Nearest Neighbor: {testAccuracy_knn1 * 100:.2f}%\")\n",
        "\n",
        "# 3-Nearest Neighbors Classifier\n",
        "knn_3 = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_3.fit(alltraindata, alltrainlabel)\n",
        "testpred_knn3 = knn_3.predict(testdata)\n",
        "testAccuracy_knn3 = accuracy_score(testlabel, testpred_knn3)\n",
        "print(f\"Test accuracy using 3-Nearest Neighbors: {testAccuracy_knn3 * 100:.2f}%\")\n",
        "```\n",
        "\n",
        " Step 3: Observe and Compare the Results\n",
        "- The output will give you the accuracy of both the 1-Nearest Neighbor and 3-Nearest Neighbors classifiers on the test dataset.\n",
        "\n",
        " Explanation:\n",
        "- **1-Nearest Neighbor Classifier**: This classifier looks for the closest training point to each test point and predicts the label of that closest point.\n",
        "- **3-Nearest Neighbors Classifier**: This classifier looks for the 3 closest training points to each test point and predicts the majority label among those 3 neighbors.\n",
        "\n",
        "Expected Outcome:\n",
        "- The 3-Nearest Neighbors classifier typically gives better generalization compared to the 1-Nearest Neighbor classifier because it considers more neighbors, which reduces the impact of noise in the data.\n",
        "- The accuracy might be slightly higher for the 3-Nearest Neighbors classifier, though it depends on the dataset.\n",
        "\n",
        "Running the Code\n",
        "You can run the above code in your environment to see the exact accuracy values for your specific dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "5a5a53ec-beea-4f7f-cefc-79f74cceb1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "ANS...:\n",
        "Yes, averaging the validation accuracy across multiple splits does generally provide more consistent and reliable results. This is a common technique in machine learning to reduce the variance of model performance estimates, making them more representative of the model's true generalization ability.\n",
        "\n",
        "Why Does Averaging Improve Consistency?\n",
        "\n",
        "1. **Reduces Variance**: When you train and validate your model on a single split, the accuracy can be overly dependent on that specific split. If that split happens to be particularly easy or difficult, the accuracy may be overly optimistic or pessimistic. By using multiple splits and averaging the results, you smooth out these inconsistencies.\n",
        "\n",
        "2. **More Robust Estimation**: Different data splits can lead to different models, especially when the dataset is small. By averaging the results across several splits (like in k-fold cross-validation), you get a better sense of how your model is likely to perform on unseen data.\n",
        "\n",
        "3. **Minimizes the Effect of Outliers**: In some splits, the validation set might contain outliers or uncommon data points that can disproportionately influence the model’s performance. By averaging across multiple splits, the effect of such outliers is diminished.\n",
        "\n",
        "### Techniques That Use Averaging Across Splits:\n",
        "\n",
        "1. **k-Fold Cross-Validation**: The dataset is split into `k` subsets. The model is trained `k` times, each time using `k-1` subsets for training and the remaining subset for validation. The final accuracy is the average accuracy across all `k` folds.\n",
        "\n",
        "2. **Repeated k-Fold Cross-Validation**: To increase reliability, k-fold cross-validation can be repeated multiple times with different random splits. The final performance is then averaged across all the repetitions.\n",
        "\n",
        "3. **Stratified k-Fold Cross-Validation**: If the dataset is imbalanced, stratified k-fold ensures each fold has roughly the same distribution of classes, leading to more consistent results.\n",
        "\n",
        "### Example of Averaging with k-Fold Cross-Validation in Scikit-Learn:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Perform 5-fold cross-validation and average the results\n",
        "cv_scores = cross_val_score(knn, alltraindata, alltrainlabel, cv=5)\n",
        "\n",
        "# Average accuracy across all splits\n",
        "average_accuracy = cv_scores.mean()\n",
        "print(f\"Average validation accuracy across 5 folds: {average_accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "### Conclusion:\n",
        "Averaging validation accuracy across multiple splits (like in k-fold cross-validation) does indeed lead to more consistent and reliable results. It gives a better estimation of how well your model will generalize to unseen data and reduces the risk of overfitting or underfitting due to a particular split.\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "ANS...:\n",
        "Yes, averaging validation accuracy across multiple splits generally gives a more accurate estimate of test accuracy. Here's why:\n",
        "\n",
        "### 1. **Generalization to Unseen Data**:\n",
        "A single train-test split might produce a model that performs very well or very poorly due to the specific characteristics of that split. By using multiple splits (like in k-fold cross-validation), you assess your model's performance across different parts of the dataset. This gives a more comprehensive view of how the model might perform on unseen data.\n",
        "\n",
        "### 2. **Reduces the Risk of Overfitting/Underfitting**:\n",
        "When you rely on a single split, your model might overfit to that specific training set or underperform if the validation set is particularly challenging. Averaging across multiple splits smooths out these extremes, giving a more accurate picture of the model’s actual performance on unseen data.\n",
        "\n",
        "### 3. **More Stable and Reliable Results**:\n",
        "In practice, a model that consistently performs well across different splits is more likely to generalize better to new data. Since k-fold cross-validation evaluates the model on different subsets, the average validation accuracy provides a more stable estimate of the model’s true performance.\n",
        "\n",
        "### 4. **Better Representation of the Dataset**:\n",
        "In k-fold cross-validation, every data point gets a chance to be in the validation set exactly once. This reduces the bias in performance estimation since all data points are eventually used for both training and validation. As a result, the average validation accuracy is a closer approximation of how the model will perform on truly unseen test data.\n",
        "\n",
        "### Real-World Application:\n",
        "In many cases, models validated with k-fold cross-validation tend to have test accuracies that are closer to the averaged validation accuracies than models validated using a single train-test split. This is why k-fold cross-validation is often preferred in machine learning pipelines.\n",
        "\n",
        "### Example of k-Fold Cross-Validation Estimating Test Accuracy:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Perform 5-fold cross-validation and average the results\n",
        "cv_scores = cross_val_score(knn, alltraindata, alltrainlabel, cv=5)\n",
        "\n",
        "# Average accuracy across all splits\n",
        "estimated_test_accuracy = cv_scores.mean()\n",
        "print(f\"Estimated test accuracy using 5-fold cross-validation: {estimated_test_accuracy * 100:.2f}%\")\n",
        "```\n",
        "\n",
        "### Summary:\n",
        "Averaging validation accuracy across multiple splits gives a more accurate and realistic estimate of test accuracy than relying on a single train-test split. It better captures how the model will perform on new, unseen data by reducing the variance and bias inherent in individual splits.\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "ANS..:\n",
        "In iterative algorithms or processes, increasing the number of iterations generally improves the estimate's accuracy. Here's how:\n",
        "\n",
        "1. **Convergence**: Many algorithms improve their estimates as they iterate, converging to a more accurate result. For example, in numerical methods, more iterations often lead to a more accurate solution.\n",
        "\n",
        "2. **Error Reduction**: More iterations can help reduce errors by refining the estimate through repeated updates. This is especially true for algorithms that incrementally approach a solution.\n",
        "\n",
        "3. **Diminishing Returns**: While more iterations usually improve accuracy, the improvements might diminish over time. After a certain point, additional iterations may offer only marginal gains.\n",
        "\n",
        "4. **Computational Cost**: More iterations increase the computational cost. There’s a trade-off between accuracy and computational efficiency.\n",
        "\n",
        "Overall, while more iterations tend to improve estimates, the actual benefit depends on the specific algorithm and problem context.\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "ANS..:\n",
        "Increasing the number of iterations can help improve the performance of a model, but it might not fully compensate for a very small training or validation dataset. Here’s why:\n",
        "\n",
        "1. **Overfitting**: With a small dataset, increasing iterations might lead to overfitting, where the model learns the noise and specific details of the training data rather than generalizing well to new data.\n",
        "\n",
        "2. **Limited Data Representation**: A small dataset might not provide enough variability or coverage of the feature space, making it hard for the model to learn meaningful patterns, regardless of how many iterations are used.\n",
        "\n",
        "3. **Validation Performance**: If the validation dataset is very small, it might not reliably reflect the model’s performance. More iterations might help, but the validation results could still be noisy or unrepresentative.\n",
        "\n",
        "4. **Regularization and Techniques**: To handle small datasets more effectively, consider using techniques like regularization, data augmentation, or cross-validation, which can help mitigate the issues associated with limited data.\n",
        "\n",
        "In summary, while increasing iterations can be beneficial, addressing the issue of a small dataset often requires additional strategies beyond just more iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "When evaluating a k-nearest neighbors (k-NN) classifier, the accuracy can be influenced by the number of splits and split size, particularly in the context of cross-validation. Here's how these factors typically affect the performance of both 1-nearest neighbor (1-NN) and 3-nearest neighbor (3-NN) classifiers:\n",
        "\n",
        "### Number of Splits\n",
        "\n",
        "1. **More Splits (e.g., k-fold Cross-Validation)**:\n",
        "   - **1-NN**: With more splits, the training set for each fold becomes smaller, which can increase variability in the performance metrics due to the smaller sample size used for training in each fold. However, because 1-NN is highly sensitive to noise and outliers, it may show higher variability in accuracy.\n",
        "   - **3-NN**: The impact of more splits is generally less severe for 3-NN compared to 1-NN. The classifier averages the predictions of three nearest neighbors, which helps mitigate the effect of individual noisy or outlier data points. This can lead to more stable accuracy metrics as the number of splits increases.\n",
        "\n",
        "2. **Fewer Splits**:\n",
        "   - **1-NN**: With fewer splits, each fold has a larger training set, which can lead to more stable estimates of accuracy. However, if the dataset is small, a single large training set might still be prone to overfitting.\n",
        "   - **3-NN**: Similar to 1-NN, fewer splits mean larger training sets per fold, which generally leads to more stable and less variable accuracy estimates. The 3-NN classifier's ability to generalize might improve compared to 1-NN due to reduced sensitivity to individual data points.\n",
        "\n",
        "### Split Size\n",
        "\n",
        "1. **Larger Split Size**:\n",
        "   - **1-NN**: Larger training sets in each fold provide more data for the classifier, which can lead to more stable and potentially higher accuracy. However, the high sensitivity of 1-NN to individual instances means that large training sets might not fully resolve its issues with overfitting and noise.\n",
        "   - **3-NN**: Larger training sets benefit 3-NN by providing a richer set of neighbors for classification, which generally leads to better generalization and more stable accuracy compared to 1-NN. The classifier is less affected by noise in larger datasets.\n",
        "\n",
        "2. **Smaller Split Size**:\n",
        "   - **1-NN**: Smaller training sets can lead to higher variance in accuracy and potential overfitting due to the classifier’s sensitivity to individual data points.\n",
        "   - **3-NN**: Smaller training sets might still provide more stable accuracy compared to 1-NN, but the classifier’s performance can be more variable due to reduced neighborhood information.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **1-NN** is more sensitive to the number of splits and split size due to its reliance on a single nearest neighbor. It is likely to show higher variability in accuracy and might be more prone to overfitting with smaller splits.\n",
        "- **3-NN** generally exhibits more stable accuracy with varying numbers of splits and split sizes, as it considers multiple neighbors for classification, reducing sensitivity to individual noisy data points.\n",
        "\n",
        "In essence, 3-NN tends to be more robust and stable across different cross-validation setups compared to 1-NN.> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}